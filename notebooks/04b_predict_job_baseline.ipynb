{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824b842-9cf8-40fb-9792-ec3b17167001",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH       = \"../data/processed/2_cleaned_data.pkl\"\n",
    "\n",
    "ROLE_COLS      = ['DevType']\n",
    "TECH_COLS      = ['LanguageWorkedWith',    'DatabaseWorkedWith',    'WebframeWorkedWith',    'MiscTechWorkedWith']\n",
    "\n",
    "EXPERIMENT_NAME = \"stackoverflow_single_model\"\n",
    "LOG_PATH = \"../models/temp/baseline/\"\n",
    "LOG_DATA_PKL    =  \"data.pkl\"\n",
    "LOG_MODEL_PKL   =  \"model.pkl\"\n",
    "LOG_METRICS_PKL =  \"metrics.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c1608-0388-4846-bef3-d6cb67ed7539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import plotly \n",
    "import os\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3f245-5fe8-4f1a-a135-a9a670419bc5",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3f45d-ebb9-43a9-a8a6-51784614b8ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6b35c-9776-4b34-b994-c00aa88c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data \n",
    "df = pd.read_pickle(DF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dc150-ba0c-48a2-9cfc-a6d259cff5c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91508beb-46d3-416f-a359-cbd9f234afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality(ground_truth, prediction, metric_function, sort_values=False):\n",
    "    quality_scores = {}\n",
    "    for col in predictions.columns:\n",
    "        role_pred  = predictions[col].copy()\n",
    "        role_truth = ground_truth[col].copy()\n",
    "        quality_scores[col] = round(metric_function(role_truth, role_pred) * 100, 2)\n",
    "        \n",
    "    quality_scores = pd.Series(quality_scores.values(), index=quality_scores.keys())\n",
    "    if sort_values:\n",
    "        quality_scores = quality_scores.sort_values()\n",
    "    \n",
    "    return quality_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d7a9c-3b9e-44bf-a360-28879aca6720",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Balance classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4498b3-c98d-43a2-89d6-9f957c3163c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total samples of roles\n",
    "roles_df = df[\"DevType\"].copy()\n",
    "role_sum = df[\"DevType\"].sum(axis=0)\n",
    "role_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9b69a-cd32-446e-a72c-31d67fb7b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample roles\n",
    "samples_per_class = 500\n",
    "resampled_roles = []\n",
    "\n",
    "for role_col in roles_df.columns:\n",
    "    sub_df = roles_df.loc[roles_df[role_col] == 1].copy()\n",
    "    \n",
    "    if len(sub_df) < samples_per_class:\n",
    "        # Upsample\n",
    "        sub_df = sub_df.sample(samples_per_class, replace=True, random_state=0)\n",
    "    else:\n",
    "        # Downsample\n",
    "        sub_df = sub_df.sample(samples_per_class, random_state=0) \n",
    "    \n",
    "    resampled_roles.append(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243ca57-c4c6-4e07-b7ee-e8e4ad5052a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dfs\n",
    "roles_df  = pd.concat(resampled_roles)\n",
    "df = df.loc[roles_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f6e9d-8322-451b-9f13-cdc3281c0a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5833db-2a42-4ee4-bc1b-343a45235f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17cc20-d76b-4d30-84c1-62aad2ad3af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop(\"DevType\", axis=1), \n",
    "                                                    df[\"DevType\"], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad322cb-3220-4a91-bd04-de8e3f05b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc6bcc-a6dc-48bb-97e4-2b2b9566a1df",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31fd6c-f625-4d5d-851c-8e6fed42bfe5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7c2b7-5dbc-4c78-a51b-a79b83e522fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db7a05-fbdf-404c-af01-d50cd3adba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client and experiment\n",
    "client = MlflowClient()\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "exp = client.get_experiment_by_name(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc2b64-c68e-460b-bfdf-8f70606ccff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f1723-a750-4924-892e-1c8422169044",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), \n",
    "                    MultiOutputClassifier(LogisticRegression()))\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "predictions =  pd.DataFrame(clf.predict(X_train), columns=Y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e791860-6e49-44ec-83b5-3f44bfdb6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "predictions =  pd.DataFrame(clf.predict(X_train), columns=Y_train.columns)\n",
    "train_scores = {score.__name__: calculate_quality(Y_train, predictions, score) \n",
    "                for score in [accuracy_score, precision_score, recall_score, f1_score]}\n",
    "train_scores = pd.concat(train_scores,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4c49e-1a77-4eec-8677-5cee37aae884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions =  pd.DataFrame(clf.predict(X_test), columns=Y_test.columns)\n",
    "test_scores = {score.__name__: calculate_quality(Y_test, predictions, score) \n",
    "                for score in [accuracy_score, precision_score, recall_score, f1_score]}\n",
    "test_scores = pd.concat(test_scores,axis=1)\n",
    "mean_test_scores = test_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab03f9-2547-4622-b7d3-b15a2ea9b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_test_scores)\n",
    "test_scores.sort_values(\"f1_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fda7cf-f4be-4f31-810f-cd25d88303d9",
   "metadata": {},
   "source": [
    "#### Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907e71a-6555-4b16-aafe-e7f616a02afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data details\n",
    "data_details = {\"data_path\": DF_PATH,\n",
    "                \"training_indices\": X_train.index.tolist(),\n",
    "                \"test_indices\":     X_test.index.tolist(), \n",
    "                \"features_names\":   X_train.columns.droplevel(0).tolist(),\n",
    "                \"targets_names\":    Y_train.columns.tolist()}\n",
    "\n",
    "with open(os.path.join(LOG_PATH, LOG_DATA_PKL), \"wb\") as output_file:\n",
    "    pickle.dump(data_details, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78902e6-eb16-41fd-afe5-7d29f51741bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = {\"model_description\": \"Baseline model: Logistic Regression \",\n",
    "         \"model_details\": str(clf),\n",
    "         \"model_object\": clf} \n",
    "\n",
    "with open(os.path.join(LOG_PATH, LOG_MODEL_PKL), \"wb\") as output_file:\n",
    "    pickle.dump(model, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e7af8-dd5a-4aee-bf80-56748e0a6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preformance details\n",
    "classes_metrics = {\"train_scores\": train_scores, \n",
    "                   \"test_scores\":  test_scores}\n",
    "\n",
    "with open(os.path.join(LOG_PATH, LOG_METRICS_PKL), \"wb\") as output_file:\n",
    "    pickle.dump(classes_metrics, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2b1c7-30fc-42cf-a952-29abfa1ac0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new run and track \n",
    "with mlflow.start_run(experiment_id=exp.experiment_id, run_name=model[\"model_description\"]):\n",
    "    # Log pickles \n",
    "    mlflow.log_artifacts(LOG_PATH)\n",
    "    \n",
    "    # Track metrics \n",
    "    for metric, score in mean_test_scores.items():\n",
    "        mlflow.log_metric(metric, score) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de382c91-f301-4aa7-81f8-9ca6bc00491a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
